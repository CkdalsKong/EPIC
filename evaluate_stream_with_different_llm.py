#!/usr/bin/env python3
"""
Stream ê²°ê³¼ë¥¼ ë‹¤ë¥¸ LLMìœ¼ë¡œ ì¬í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
EPIC_stream.pyë¡œ ìƒì„±í•œ ê²°ê³¼ë¥¼ evaluation_with_different_llm.pyë¡œ í‰ê°€
"""

import os
import json
import csv
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from tqdm import tqdm
from datetime import datetime

try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("âš ï¸ matplotlib not installed. Plotting functions will be disabled.")

from evaluation_with_different_llm import EvaluationWithDifferentLLM


class StreamEvaluator:
    """Stream ê²°ê³¼ë¥¼ ë‹¤ë¥¸ LLMìœ¼ë¡œ ì¬í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, evaluator: EvaluationWithDifferentLLM):
        """
        Args:
            evaluator: EvaluationWithDifferentLLM ì¸ìŠ¤í„´ìŠ¤
        """
        self.evaluator = evaluator
    
    def find_stream_directories(self, base_dir: str) -> List[str]:
        """
        Stream ë””ë ‰í† ë¦¬ë“¤ì„ ì°¾ê¸°
        
        Args:
            base_dir: ë©”ì†Œë“œ ë””ë ‰í† ë¦¬ (ì˜ˆ: output_prefeval/EPIC_inst/1)
        
        Returns:
            List of stream directory paths
        """
        base_path = Path(base_dir)
        if not base_path.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
            return []
        
        # stream_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
        stream_dirs = []
        for item in base_path.iterdir():
            if item.is_dir() and item.name.startswith("stream_"):
                stream_dirs.append(str(item))
        
        # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
        stream_dirs.sort(reverse=True)
        
        if not stream_dirs:
            print(f"âš ï¸ Stream ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}")
        else:
            print(f"âœ… {len(stream_dirs)}ê°œì˜ Stream ë””ë ‰í† ë¦¬ ë°œê²¬")
            for sd in stream_dirs:
                print(f"   - {os.path.basename(sd)}")
        
        return stream_dirs
    
    def find_checkpoints(self, stream_dir: str) -> List[Dict[str, Any]]:
        """
        Stream ë””ë ‰í† ë¦¬ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë“¤ ì°¾ê¸°
        
        Args:
            stream_dir: Stream ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
        Returns:
            List of checkpoint info dicts: [{"id": int, "dir": str, "generation_file": str}]
        """
        stream_path = Path(stream_dir)
        checkpoints = []
        
        # checkpoint_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
        for item in stream_path.iterdir():
            if item.is_dir() and item.name.startswith("checkpoint_"):
                checkpoint_id = int(item.name.split("_")[1])
                generation_file = item / "generation_results.json"
                
                if generation_file.exists():
                    checkpoints.append({
                        "id": checkpoint_id,
                        "dir": str(item),
                        "generation_file": str(generation_file)
                    })
        
        # ì²´í¬í¬ì¸íŠ¸ ID ìˆœìœ¼ë¡œ ì •ë ¬
        checkpoints.sort(key=lambda x: x["id"])
        
        print(f"âœ… {len(checkpoints)}ê°œì˜ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬")
        for cp in checkpoints:
            print(f"   - Checkpoint {cp['id']}: {os.path.basename(cp['dir'])}")
        
        return checkpoints
    
    def evaluate_checkpoint(self, checkpoint_info: Dict[str, Any], 
                           overwrite: bool = False) -> Optional[Dict[str, Any]]:
        """
        ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ í‰ê°€
        
        Args:
            checkpoint_info: ì²´í¬í¬ì¸íŠ¸ ì •ë³´
            overwrite: ê¸°ì¡´ í‰ê°€ ê²°ê³¼ë¥¼ ë®ì–´ì“¸ì§€ ì—¬ë¶€
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë©”íŠ¸ë¦­ ë˜ëŠ” None
        """
        checkpoint_id = checkpoint_info["id"]
        checkpoint_dir = checkpoint_info["dir"]
        generation_file = checkpoint_info["generation_file"]
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        output_file = os.path.join(checkpoint_dir, "generation_results_evaluated.json")
        
        # ì´ë¯¸ í‰ê°€ëœ ê²½ìš°
        if os.path.exists(output_file) and not overwrite:
            print(f"   â­ï¸ Checkpoint {checkpoint_id} ì´ë¯¸ í‰ê°€ë¨ (ê±´ë„ˆë›°ê¸°)")
            try:
                stats = self.evaluator.analyze_evaluation_results(output_file)
                return {
                    "checkpoint_id": checkpoint_id,
                    "stats": stats,
                    "evaluated_file": output_file
                }
            except Exception as e:
                print(f"   âš ï¸ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨, ì¬í‰ê°€: {str(e)}")
        
        print(f"   ğŸ”„ Checkpoint {checkpoint_id} í‰ê°€ ì¤‘...")
        
        # í‰ê°€ ìˆ˜í–‰
        evaluated_file = self.evaluator.evaluate_generation_file(
            generation_file, 
            output_file
        )
        
        if not evaluated_file:
            print(f"   âŒ Checkpoint {checkpoint_id} í‰ê°€ ì‹¤íŒ¨")
            return None
        
        # ê²°ê³¼ ë¶„ì„
        stats = self.evaluator.analyze_evaluation_results(evaluated_file)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚° (EPIC_stream.py í˜•ì‹ì— ë§ì¶¤)
        total = stats.get("total_responses", 0)
        if total == 0:
            print(f"   âš ï¸ Checkpoint {checkpoint_id}: í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # Stream í˜•ì‹ì˜ ë©”íŠ¸ë¦­ ìƒì„±
        metrics = {
            "checkpoint_id": checkpoint_id,
            "unhelpful": stats.get("error_unhelpful", 0),
            "inconsistent": stats.get("error_inconsistent", 0),
            "hallucination_of_preference_violation": stats.get("hallucination_of_preference_violation", 0),
            "preference_unaware_violation": stats.get("preference_unaware_violation", 0),
            "preference_following_accuracy": stats.get("preference_following_accuracy_percent", 0),
            "total_responses": total,
            "evaluated_file": evaluated_file
        }
        
        print(f"   âœ… Checkpoint {checkpoint_id} í‰ê°€ ì™„ë£Œ - ì •í™•ë„: {metrics['preference_following_accuracy']:.2f}%")
        
        return metrics
    
    def load_stream_metadata(self, stream_dir: str) -> Dict[str, Any]:
        """Stream ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        meta_file = os.path.join(stream_dir, "stream_metadata.json")
        if os.path.exists(meta_file):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return {}
    
    def load_preference_history(self, stream_dir: str) -> List[Dict[str, Any]]:
        """Preference íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        history_file = os.path.join(stream_dir, "preference_history.json")
        if os.path.exists(history_file):
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ Preference íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return []
    
    def load_checkpoint_metadata(self, checkpoint_dir: str) -> Dict[str, Any]:
        """ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ (docs_processed ë“±)"""
        metrics_file = os.path.join(checkpoint_dir, "metrics.json")
        if os.path.exists(metrics_file):
            try:
                with open(metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return {}
    
    def evaluate_stream(self, stream_dir: str, overwrite: bool = False) -> Dict[str, Any]:
        """
        Stream ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ í‰ê°€
        
        Args:
            stream_dir: Stream ë””ë ‰í† ë¦¬ ê²½ë¡œ
            overwrite: ê¸°ì¡´ í‰ê°€ ê²°ê³¼ë¥¼ ë®ì–´ì“¸ì§€ ì—¬ë¶€
        
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ Stream ì¬í‰ê°€ ì‹œì‘: {os.path.basename(stream_dir)}")
        print(f"{'='*60}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        checkpoints = self.find_checkpoints(stream_dir)
        if not checkpoints:
            print("âŒ í‰ê°€í•  ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {}
        
        # Stream ë©”íƒ€ë°ì´í„° ë¡œë“œ
        stream_meta = self.load_stream_metadata(stream_dir)
        preference_history = self.load_preference_history(stream_dir)
        
        # ê° ì²´í¬í¬ì¸íŠ¸ í‰ê°€
        checkpoint_results = []
        for checkpoint_info in tqdm(checkpoints, desc="ì²´í¬í¬ì¸íŠ¸ í‰ê°€"):
            # ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ (docs_processed ë“±)
            checkpoint_meta = self.load_checkpoint_metadata(checkpoint_info["dir"])
            
            # í‰ê°€ ìˆ˜í–‰
            metrics = self.evaluate_checkpoint(checkpoint_info, overwrite)
            
            if metrics:
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                metrics["docs_processed"] = checkpoint_meta.get("docs_processed", 0)
                metrics["total_indexed"] = checkpoint_meta.get("total_indexed", 0)
                metrics["active_chunks"] = checkpoint_meta.get("active_chunks", 0)
                metrics["active_preferences"] = checkpoint_meta.get("active_preferences", 0)
                metrics["timestamp"] = datetime.now().isoformat()
                
                checkpoint_results.append(metrics)
        
        # ê²°ê³¼ ì €ì¥
        results_file = os.path.join(stream_dir, "all_checkpoints_reevaluated.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ì¬í‰ê°€ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # CSV ìƒì„±
        self._generate_summary_csv(stream_dir, checkpoint_results)
        
        # ê·¸ë˜í”„ ìƒì„±
        if HAS_MATPLOTLIB:
            self.plot_stream_results(stream_dir, checkpoint_results, preference_history)
        
        return {
            "stream_dir": stream_dir,
            "checkpoints": checkpoint_results,
            "metadata": stream_meta
        }
    
    def _generate_summary_csv(self, stream_dir: str, checkpoint_results: List[Dict[str, Any]]):
        """CSV ìš”ì•½ ìƒì„±"""
        csv_file = os.path.join(stream_dir, "checkpoint_summary_reevaluated.csv")
        
        fieldnames = [
            "checkpoint_id",
            "docs_processed",
            "total_indexed",
            "active_chunks",
            "active_preferences",
            "unhelpful",
            "inconsistent",
            "hallucination_of_preference_violation",
            "preference_unaware_violation",
            "preference_following_accuracy",
            "total_responses",
            "timestamp"
        ]
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for checkpoint in checkpoint_results:
                row = {field: checkpoint.get(field, "") for field in fieldnames}
                writer.writerow(row)
        
        print(f"ğŸ“Š ì¬í‰ê°€ CSV ì €ì¥: {csv_file}")
    
    def plot_stream_results(self, stream_dir: str, checkpoint_results: List[Dict[str, Any]], 
                           preference_history: List[Dict[str, Any]]):
        """
        Stream ê²°ê³¼ ê·¸ë˜í”„ ìƒì„± (EPIC_stream.pyì™€ ë™ì¼í•œ í˜•ì‹)
        """
        if not HAS_MATPLOTLIB:
            print("âš ï¸ matplotlib not installed. Cannot generate plots.")
            return
        
        if not checkpoint_results:
            print("âš ï¸ No checkpoint results to plot.")
            return
        
        # ë°ì´í„° ì¶”ì¶œ
        docs_processed = [cp.get("docs_processed", 0) for cp in checkpoint_results]
        
        metrics = {
            "Unhelpful": [cp.get("unhelpful", 0) for cp in checkpoint_results],
            "Inconsistent": [cp.get("inconsistent", 0) for cp in checkpoint_results],
            "Hallucination Violation": [cp.get("hallucination_of_preference_violation", 0) for cp in checkpoint_results],
            "Unaware Violation": [cp.get("preference_unaware_violation", 0) for cp in checkpoint_results],
            "Accuracy (%)": [cp.get("preference_following_accuracy", 0) for cp in checkpoint_results]
        }
        
        # ê·¸ë˜í”„ 1: 2ê°œ subplot (Error metrics + Accuracy)
        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)
        
        colors = ['#e74c3c', '#f39c12', '#9b59b6', '#3498db', '#2ecc71']
        
        # Plot 1: Error metrics (counts)
        ax1 = axes[0]
        for i, (metric_name, values) in enumerate(list(metrics.items())[:-1]):
            ax1.plot(docs_processed, values, marker='o', label=metric_name,
                    color=colors[i], linewidth=2, markersize=6)
        
        ax1.set_ylabel('Error Count', fontsize=12)
        ax1.set_title('Stream Evaluation (Re-evaluated): Error Metrics Over Time', fontsize=14, fontweight='bold')
        ax1.legend(loc='upper left', fontsize=10)
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Accuracy
        ax2 = axes[1]
        ax2.plot(docs_processed, metrics["Accuracy (%)"], marker='s',
                color=colors[4], linewidth=2, markersize=8, label='Preference Following Accuracy')
        ax2.fill_between(docs_processed, metrics["Accuracy (%)"], alpha=0.3, color=colors[4])
        
        ax2.set_xlabel('Documents Processed', fontsize=12)
        ax2.set_ylabel('Accuracy (%)', fontsize=12)
        ax2.set_title('Stream Evaluation (Re-evaluated): Preference Following Accuracy', fontsize=14, fontweight='bold')
        ax2.legend(loc='lower right', fontsize=10)
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 100)
        
        # Preference events ì¶”ê°€
        add_events = [e for e in preference_history if e.get("type") == "add"]
        remove_events = [e for e in preference_history if e.get("type") == "remove"]
        
        for ax in axes:
            for event in add_events:
                docs_at_event = event.get("docs_processed", 0)
                if docs_at_event > 0:
                    ax.axvline(x=docs_at_event, color='green', linestyle='--',
                              alpha=0.7, linewidth=1.5)
            for event in remove_events:
                docs_at_event = event.get("docs_processed", 0)
                if docs_at_event > 0:
                    ax.axvline(x=docs_at_event, color='red', linestyle='--',
                              alpha=0.7, linewidth=1.5)
        
        # Legend for events
        if add_events or remove_events:
            add_patch = mpatches.Patch(color='green', alpha=0.7, label='Preference Added')
            remove_patch = mpatches.Patch(color='red', alpha=0.7, label='Preference Removed')
            patches = []
            if add_events:
                patches.append(add_patch)
            if remove_events:
                patches.append(remove_patch)
            if patches:
                axes[0].legend(handles=list(axes[0].get_legend_handles_labels()[0]) + patches,
                              loc='upper left', fontsize=9)
        
        plt.tight_layout()
        
        # Save plot
        plot_file = os.path.join(stream_dir, "stream_evaluation_reevaluated_plot.png")
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ ê·¸ë˜í”„ ì €ì¥: {plot_file}")
        
        # Combined metrics plot
        self._plot_combined_metrics(stream_dir, docs_processed, metrics, preference_history)
    
    def _plot_combined_metrics(self, stream_dir: str, docs_processed: List[int],
                              metrics: Dict[str, List[float]], preference_history: List[Dict[str, Any]]):
        """Combined metrics plot ìƒì„±"""
        if not HAS_MATPLOTLIB:
            return
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        colors = ['#e74c3c', '#f39c12', '#9b59b6', '#3498db', '#2ecc71']
        markers = ['o', 's', '^', 'D', 'v']
        
        # Normalize error counts to percentage
        max_errors = max(
            max(metrics["Unhelpful"]) if metrics["Unhelpful"] else 1,
            max(metrics["Inconsistent"]) if metrics["Inconsistent"] else 1,
            max(metrics["Hallucination Violation"]) if metrics["Hallucination Violation"] else 1,
            max(metrics["Unaware Violation"]) if metrics["Unaware Violation"] else 1,
            1
        )
        
        for i, (metric_name, values) in enumerate(metrics.items()):
            if metric_name == "Accuracy (%)":
                plot_values = values
            else:
                # Invert and normalize: fewer errors = higher score
                plot_values = [100 - (v / max_errors * 100) if max_errors > 0 else 100 for v in values]
                metric_name = f"No {metric_name} (%)"
            
            ax.plot(docs_processed, plot_values, marker=markers[i], label=metric_name,
                   color=colors[i], linewidth=2, markersize=6)
        
        # Add preference events
        for event in preference_history:
            docs_at_event = event.get("docs_processed", 0)
            if docs_at_event > 0:
                color = 'green' if event.get("type") == "add" else 'red'
                ax.axvline(x=docs_at_event, color=color, linestyle='--', alpha=0.5)
        
        ax.set_xlabel('Documents Processed', fontsize=12)
        ax.set_ylabel('Score (%)', fontsize=12)
        ax.set_title('Stream Evaluation (Re-evaluated): All Metrics (Higher is Better)', fontsize=14, fontweight='bold')
        ax.legend(loc='lower right', fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 105)
        
        plt.tight_layout()
        
        plot_file = os.path.join(stream_dir, "stream_combined_reevaluated_plot.png")
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Combined ê·¸ë˜í”„ ì €ì¥: {plot_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Stream ê²°ê³¼ë¥¼ ë‹¤ë¥¸ LLMìœ¼ë¡œ ì¬í‰ê°€')
    parser.add_argument('--stream_dir', type=str, default=None,
                       help='Stream ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ base_dirì—ì„œ ìë™ ê²€ìƒ‰)')
    parser.add_argument('--base_dir', type=str, default=None,
                       help='ë©”ì†Œë“œ ë””ë ‰í† ë¦¬ (ì˜ˆ: output_prefeval/EPIC_inst/1)')
    parser.add_argument('--vllm_url', type=str, default='http://localhost:8011',
                       help='vLLM ì„œë²„ URL')
    parser.add_argument('--eval_model', type=str, default='meta-llama/Llama-3.3-70B-Instruct',
                       help='í‰ê°€ ëª¨ë¸')
    parser.add_argument('--overwrite', action='store_true',
                       help='ê¸°ì¡´ í‰ê°€ ê²°ê³¼ë¥¼ ë®ì–´ì“°ê¸°')
    
    args = parser.parse_args()
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = EvaluationWithDifferentLLM(
        vllm_base_url=args.vllm_url,
        evaluation_model=args.eval_model,
        max_tokens=512,
        temperature=0.0,
        timeout=60,
        retry_count=3
    )
    
    stream_evaluator = StreamEvaluator(evaluator)
    
    # Stream ë””ë ‰í† ë¦¬ ì°¾ê¸°
    if args.stream_dir:
        stream_dirs = [args.stream_dir]
    elif args.base_dir:
        stream_dirs = stream_evaluator.find_stream_directories(args.base_dir)
    else:
        print("âŒ --stream_dir ë˜ëŠ” --base_dir ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤")
        return
    
    if not stream_dirs:
        print("âŒ í‰ê°€í•  Stream ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ê° Stream ë””ë ‰í† ë¦¬ í‰ê°€
    for stream_dir in stream_dirs:
        try:
            stream_evaluator.evaluate_stream(stream_dir, overwrite=args.overwrite)
        except Exception as e:
            print(f"âŒ Stream í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
    
    print("\nâœ… ëª¨ë“  Stream ì¬í‰ê°€ ì™„ë£Œ")


if __name__ == "__main__":
    main()

