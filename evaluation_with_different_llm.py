#!/usr/bin/env python3
"""
ë‹¤ë¥¸ LLMì„ ì‚¬ìš©í•œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ë¡œì»¬ vLLM ì„œë²„ ì‚¬ìš©
"""

import os
import json
import requests
import time
import csv
import re
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple, Union
import argparse

class EvaluationWithDifferentLLM:
    def __init__(self, 
                 vllm_base_url: str = "http://localhost:8011",
                 evaluation_model: str = "meta-llama/Llama-3.3-70B-Instruct",
                 max_tokens: int = 512,
                 temperature: float = 0.1,
                 timeout: int = 60,
                 retry_count: int = 3):
        """
        ë‹¤ë¥¸ LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë¬¸ì„œë¥¼ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤
        
        Args:
            vllm_base_url: vLLM ì„œë²„ URL (ë¡œì»¬ ì„œë²„)
            evaluation_model: í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ëª…
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            timeout: API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            retry_count: ì¬ì‹œë„ íšŸìˆ˜
        """
        self.vllm_base_url = vllm_base_url.rstrip('/')
        self.evaluation_model = evaluation_model
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = timeout
        self.retry_count = retry_count
        
        # í‰ê°€ ë©”íŠ¸ë¦­ íŒŒì¼ ê²½ë¡œ
        self.error_type_dir = "data/error_type"
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        self.check_server_connection()
        
    
    def check_server_connection(self) -> bool:
        """vLLM ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            # ì„œë²„ ìƒíƒœ í™•ì¸
            health_url = f"{self.vllm_base_url}/health"
            response = requests.get(health_url, timeout=10)
            
            if response.status_code == 200:
                print(f"âœ… vLLM ì„œë²„ ì—°ê²° ì„±ê³µ: {self.vllm_base_url}")
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
                models_url = f"{self.vllm_base_url}/v1/models"
                models_response = requests.get(models_url, timeout=10)
                
                if models_response.status_code == 200:
                    models_data = models_response.json()
                    available_models = [model["id"] for model in models_data.get("data", [])]
                    print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
                    
                    if self.evaluation_model not in available_models:
                        print(f"âš ï¸ ê²½ê³ : {self.evaluation_model} ëª¨ë¸ì´ ì„œë²„ì— ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”: {available_models}")
                        return False
                    else:
                        print(f"âœ… í‰ê°€ ëª¨ë¸ í™•ì¸ë¨: {self.evaluation_model}")
                        return True
                else:
                    print(f"âš ï¸ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {models_response.status_code}")
                    return True  # ëª¨ë¸ ëª©ë¡ ì¡°íšŒëŠ” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            else:
                print(f"âŒ vLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print(f"âŒ vLLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.vllm_base_url}")
            print("   ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False
        except requests.exceptions.Timeout:
            print(f"âŒ vLLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼: {self.vllm_base_url}")
            return False
        except Exception as e:
            print(f"âŒ ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
        
    def load_evaluation_prompts(self) -> Dict[str, str]:
        """í‰ê°€ ë©”íŠ¸ë¦­ í”„ë¡¬í”„íŠ¸ë“¤ì„ ë¡œë“œ"""
        file_dict = {
            "acknow": "check_acknowledge.txt",
            "violate": "check_violation.txt", 
            "hallucinate": "check_hallucination.txt",
            "helpful": "check_helpful.txt"
        }
        
        prompts = {}
        for metric_name, file_name in file_dict.items():
            file_path = os.path.join(self.error_type_dir, file_name)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    prompts[metric_name] = f.read()
            except FileNotFoundError:
                print(f"âš ï¸ Warning: {file_path} not found")
                prompts[metric_name] = ""
                
        return prompts
    
    def call_vllm_api(self, messages: List[Dict[str, str]], system_prompt: str = "") -> Optional[str]:
        """vLLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(self.retry_count):
            try:
                payload = {
                    "model": self.evaluation_model,
                    "messages": messages,
                    "temperature": 0.0,
                    "max_tokens": self.max_tokens,
                    "seed": 0,
                    "top_p": 1.0,
                    "top_k": -1,
                    "frequency_penalty": 0.0,
                    "presence_penalty": 0.0,
                    "stream": False,
                    "dtype": "float32"
                }
                
                if system_prompt:
                    payload["system"] = system_prompt
                
                # Add extra_body only for OSS model
                if self.evaluation_model == "openai/gpt-oss-20b":
                    payload["extra_body"] = {"reasoning_effort": "low"}
                
                response = requests.post(
                    f"{self.vllm_base_url}/v1/chat/completions",
                    json=payload,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    return response.json()["choices"][0]["message"]["content"]
                elif response.status_code == 503:
                    print(f"âš ï¸ ì„œë²„ ê³¼ë¶€í•˜ (503), ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{self.retry_count})")
                    if attempt < self.retry_count - 1:
                        import time
                        time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                elif response.status_code == 400:
                    # í† í° ê¸¸ì´ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                    error_text = response.text.lower()
                    if "context length" in error_text or "token" in error_text:
                        print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼, ë©”ì‹œì§€ ë‹¨ì¶• ì‹œë„... ({attempt + 1}/{self.retry_count})")
                        # ë©”ì‹œì§€ ë‚´ìš©ì„ ë‹¨ì¶•
                        shortened_messages = self.shorten_messages(messages)
                        if shortened_messages != messages:
                            messages = shortened_messages
                            continue
                    else:
                        print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                        return None
                else:
                    print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                    return None
                    
            except requests.exceptions.Timeout:
                print(f"âš ï¸ API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{self.retry_count})")
                if attempt < self.retry_count - 1:
                    import time
                    time.sleep(2 ** attempt)
                    continue
            except requests.exceptions.ConnectionError:
                print(f"âŒ ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {self.vllm_base_url}")
                return None
            except Exception as e:
                print(f"âŒ vLLM API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                return None
        
        print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ({self.retry_count})")
        return None
    
    def parse_explanation_and_answer(self, response: str) -> Tuple[str, str]:
        """ì‘ë‹µì—ì„œ ì„¤ëª…ê³¼ ë‹µë³€ì„ íŒŒì‹±"""
        try:
            # XML í˜•ì‹ íŒŒì‹± ì‹œë„
            if "<explanation>" in response and "<answer>" in response:
                explanation_start = response.find("<explanation>") + len("<explanation>")
                explanation_end = response.find("</explanation>")
                answer_start = response.find("<answer>") + len("<answer>")
                answer_end = response.find("</answer>")
                
                explanation = response[explanation_start:explanation_end].strip()
                answer = response[answer_start:answer_end].strip()
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì‹±
                lines = response.strip().split('\n')
                explanation = ""
                answer = ""
                
                for line in lines:
                    if line.lower().startswith(('yes', 'no')):
                        answer = line.strip()
                    else:
                        explanation += line.strip() + " "
                
                explanation = explanation.strip()
                
            return explanation, answer
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return "", response.strip()
    
    def parse_preference_and_answer(self, response: str) -> Tuple[str, str]:
        """preference ì¶”ì¶œê³¼ ë‹µë³€ì„ íŒŒì‹±"""
        try:
            if "<extract_preference>" in response and "<answer>" in response:
                pref_start = response.find("<extract_preference>") + len("<extract_preference>")
                pref_end = response.find("</extract_preference>")
                answer_start = response.find("<answer>") + len("<answer>")
                answer_end = response.find("</answer>")
                
                extract_pref = response[pref_start:pref_end].strip()
                answer = response[answer_start:answer_end].strip()
            else:
                extract_pref = ""
                answer = response.strip()
                
            return extract_pref, answer
            
        except Exception as e:
            print(f"âš ï¸ preference íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return "", response.strip()
    
    def evaluate_single_metric(self, task: Dict[str, Any], metric: str, 
                             eval_prompt: str, system_prompt: str) -> Optional[Tuple[str, Dict[str, str]]]:
        """ë‹¨ì¼ ë©”íŠ¸ë¦­ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰"""
        try:
            preference = task.get("preference", "")
            question = task.get("question", "")
            response = task.get("response_to_q", "")
            
            if not response:
                return None
            
            # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì¤€ë¹„
            eval_text = eval_prompt
            if metric == "acknow":
                eval_text = eval_text.replace("{end_generation}", response).replace("{question}", question)
            elif metric in ["violate", "helpful"]:
                eval_text = eval_text.replace("{preference}", preference).replace("{question}", question).replace("{end_generation}", response)
            elif metric == "hallucinate":
                # acknowledge ê²°ê³¼ê°€ í•„ìš”
                error_check = task.get("evaluation_error_analysis", {})
                if "acknow" not in error_check:
                    return None
                extracted_pref = error_check["acknow"].get("extract_pref", "")
                eval_text = eval_text.replace("{preference}", preference).replace("{assistant_restatement}", extracted_pref)
            
            # vLLM API í˜¸ì¶œ
            messages = [{"role": "user", "content": eval_text}]
            eval_response = self.call_vllm_api(messages, system_prompt)
            
            if not eval_response:
                return None
            
            # ê²°ê³¼ íŒŒì‹±
            result = {}
            if metric != "acknow":
                explanation, answer = self.parse_explanation_and_answer(eval_response)
                result["explanation"] = explanation
                result["answer"] = answer
            else:
                extract_preference, answer = self.parse_preference_and_answer(eval_response)
                result["answer"] = answer
                result["extract_pref"] = extract_preference
            
            return metric, result
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
        
    def evaluate_generation_file(self, generation_file: str, output_file: Optional[str] = None) -> str:
        """ìƒì„± íŒŒì¼ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""
        print(f"ğŸ” í‰ê°€ ì‹œì‘: {generation_file}")
        print(f"ğŸŒ ì‚¬ìš© ì„œë²„: {self.vllm_base_url}")
        print(f"ğŸ¤– í‰ê°€ ëª¨ë¸: {self.evaluation_model}")
        
        # ìƒì„± ë°ì´í„° ë¡œë“œ
        try:
            with open(generation_file, 'r', encoding='utf-8') as f:
                generation_data = json.load(f)
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return ""
        
        print(f"âœ… {len(generation_data)}ê°œì˜ ìƒì„± ê²°ê³¼ ë¡œë“œë¨")
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        eval_prompts = self.load_evaluation_prompts()
        system_prompt = "You are a helpful assistant in evaluating an AI assistant's response. You should be fair and strict and follow the user's instruction."
        
        # ì¶œë ¥ íŒŒì¼ ì„¤ì •
        if output_file is None:
            base_name = os.path.splitext(generation_file)[0]
            output_file = f"{base_name}_evaluated.json"
        
        # ë©”íŠ¸ë¦­ ìˆœì„œ ì •ì˜ (acknowê°€ ë¨¼ì € ì™€ì•¼ hallucinateê°€ ì‘ë™)
        metric_order = ["acknow", "violate", "helpful", "hallucinate"]
        
        # ê° ë©”íŠ¸ë¦­ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
        for metric in metric_order:
            if metric not in eval_prompts or not eval_prompts[metric]:
                print(f"âš ï¸ {metric} í”„ë¡¬í”„íŠ¸ê°€ ì—†ì–´ì„œ ê±´ë„ˆë›°ê¸°")
                continue
                
            print(f"ğŸ”„ {metric} ë©”íŠ¸ë¦­ í‰ê°€ ì¤‘...")
            
            # í˜„ì¬ ë©”íŠ¸ë¦­ì— ëŒ€í•´ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=1) as executor:
                futures = []
                
                for task_id, task in enumerate(generation_data):
                    if "response_to_q" not in task:
                        continue
                    
                    # ì´ë¯¸ í˜„ì¬ ë©”íŠ¸ë¦­ì´ í‰ê°€ëœ í•­ëª© ê±´ë„ˆë›°ê¸°
                    if "evaluation_error_analysis" in task:
                        analysis = task["evaluation_error_analysis"]
                        if metric in analysis:
                            continue
                    
                    # í˜„ì¬ ë©”íŠ¸ë¦­ì— ëŒ€í•´ í‰ê°€
                    future = executor.submit(
                        self.evaluate_single_metric,
                        task,
                        metric,
                        eval_prompts[metric],
                        system_prompt
                    )
                    futures.append((task_id, future))
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for task_id, future in tqdm(futures, desc=f"{metric} í‰ê°€ ì¤‘"):
                    result = future.result()
                    if result:
                        metric_name, error_check = result
                        if "evaluation_error_analysis" not in generation_data[task_id]:
                            generation_data[task_id]["evaluation_error_analysis"] = {}
                        generation_data[task_id]["evaluation_error_analysis"][metric_name] = error_check
                    else:
                        print(f"âš ï¸ {metric} í‰ê°€ ì‹¤íŒ¨: task_id {task_id}")
        
        # preference_following_accuracy ì¶”ê°€
        for task in generation_data:
            if "evaluation_error_analysis" in task:
                pfa = self.calculate_preference_following_accuracy(task["evaluation_error_analysis"])
                task["preference_following_accuracy"] = pfa
        
        # ê²°ê³¼ ì €ì¥
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(generation_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {output_file}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return output_file
    
    def analyze_evaluation_results(self, evaluated_file: str) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ë¶„ì„"""
        try:
            with open(evaluated_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return {}
        
        stats: Dict[str, Any] = {
            "total_responses": len(data),
            "acknowledgement": 0,
            "hallucination": 0,
            "violation": 0,
            "error_unhelpful": 0,
            "error_inconsistent": 0,
            "hallucination_of_preference_violation": 0,
            "preference_unaware_violation": 0,
            "preference_adherence_accuracy": 0,
        }
        
        for entry in data:
            if "evaluation_error_analysis" not in entry:
                continue
            
            error_types = entry["evaluation_error_analysis"]
            is_acknowledgement = "yes" in error_types.get("acknow", {}).get("answer", "").lower()
            is_hallucination = is_acknowledgement and "yes" in error_types.get("hallucinate", {}).get("answer", "").lower()
            is_violation = "yes" in error_types.get("violate", {}).get("answer", "").lower()
            is_unhelpful = "no" in error_types.get("helpful", {}).get("answer", "").lower()
            
            is_inconsistent = is_acknowledgement and not is_hallucination and is_violation and not is_unhelpful
            is_hallucination_of_preference_violation = (
                is_acknowledgement and is_hallucination and is_violation and not is_unhelpful
            )
            is_preference_unaware_violation = not is_acknowledgement and is_violation and not is_unhelpful
            
            preference_following_accuracy = not any([
                is_inconsistent, is_hallucination_of_preference_violation, 
                is_preference_unaware_violation, is_unhelpful
            ])
            
            stats["acknowledgement"] += is_acknowledgement
            stats["hallucination"] += is_hallucination
            stats["violation"] += is_violation
            stats["error_unhelpful"] += is_unhelpful
            stats["error_inconsistent"] += is_inconsistent
            stats["hallucination_of_preference_violation"] += is_hallucination_of_preference_violation
            stats["preference_unaware_violation"] += is_preference_unaware_violation
            stats["preference_adherence_accuracy"] += preference_following_accuracy
        
        # ë°±ë¶„ìœ¨ ê³„ì‚°
        total = stats["total_responses"]
        if total > 0:
            stats["preference_following_accuracy_percent"] = round((stats["preference_adherence_accuracy"] / total) * 100, 2)
            stats["acknowledgement_percent"] = round((stats["acknowledgement"] / total) * 100, 2)
            stats["violation_percent"] = round((stats["violation"] / total) * 100, 2)
            stats["unhelpful_percent"] = round((stats["error_unhelpful"] / total) * 100, 2)
        
        return stats
    
    def calculate_preference_following_accuracy(self, error_analysis: Dict[str, Any]) -> int:
        """preference_following_accuracy ê³„ì‚° (0 or 1)"""
        if not error_analysis:
            return 0
        
        is_acknowledgement = "yes" in error_analysis.get("acknow", {}).get("answer", "").lower()
        is_hallucination = is_acknowledgement and "yes" in error_analysis.get("hallucinate", {}).get("answer", "").lower()
        is_violation = "yes" in error_analysis.get("violate", {}).get("answer", "").lower()
        is_unhelpful = "no" in error_analysis.get("helpful", {}).get("answer", "").lower()
        
        is_inconsistent = is_acknowledgement and not is_hallucination and is_violation and not is_unhelpful
        is_hallucination_of_preference_violation = (
            is_acknowledgement and is_hallucination and is_violation and not is_unhelpful
        )
        is_preference_unaware_violation = not is_acknowledgement and is_violation and not is_unhelpful
        
        preference_following_accuracy = not any([
            is_inconsistent, 
            is_hallucination_of_preference_violation, 
            is_preference_unaware_violation, 
            is_unhelpful
        ])
        
        return 1 if preference_following_accuracy else 0

def evaluate_organized_files(organized_dir, evaluator, method_filter=None, dataset_filter=None, model_filter=None):
    """
    ì •ë¦¬ëœ íŒŒì¼ë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ í‰ê°€ ìˆ˜í–‰
    """
    from pathlib import Path
    
    organized_path = Path(organized_dir)
    if not organized_path.exists():
        print(f"âŒ ì •ë¦¬ëœ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {organized_dir}")
        return
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥ìš©
    all_results = []
    
    # ê° ë©”ì†Œë“œë³„ë¡œ ìˆœíšŒ
    for method_dir in organized_path.iterdir():
        if not method_dir.is_dir():
            continue
            
        method_name = method_dir.name
        
        # ë©”ì†Œë“œ í•„í„°ë§
        if method_filter and method_name != method_filter:
            continue
            
        print(f"\nğŸ” ë©”ì†Œë“œ '{method_name}' í‰ê°€ ì‹œì‘")
        print("="*60)
        
        method_results = []
        
        # ê° ë°ì´í„°ì…‹ë³„ë¡œ ìˆœíšŒ
        for dataset_dir in method_dir.iterdir():
            if not dataset_dir.is_dir():
                continue
                
            dataset_name = dataset_dir.name
            
            # ë°ì´í„°ì…‹ í•„í„°ë§
            if dataset_filter and dataset_name != dataset_filter:
                continue
                
            print(f"\nğŸ“Š ë°ì´í„°ì…‹ '{dataset_name}' ì²˜ë¦¬ ì¤‘...")
            
            # ë°ì´í„°ì…‹ë³„ CSV íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            dataset_csv_file = dataset_dir / f"{dataset_name}.csv"
            if dataset_csv_file.exists():
                print(f"    â­ï¸ ë°ì´í„°ì…‹ CSV íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {dataset_csv_file}")
                # ê¸°ì¡´ CSVì—ì„œ ê²°ê³¼ë¥¼ ì½ì–´ì™€ì„œ ì‚¬ìš©
                existing_results = load_existing_dataset_results(dataset_csv_file, method_name, dataset_name)
                if existing_results:
                    dataset_results = existing_results
                    method_results.extend(dataset_results)
                    all_results.extend(dataset_results)
                    print(f"    âœ… ê¸°ì¡´ ê²°ê³¼ {len(existing_results)}ê°œ ë¡œë“œë¨")
                    continue
            
            dataset_results = []
            
            # ê° ëª¨ë¸ë³„ë¡œ ìˆœíšŒ
            for model_dir in dataset_dir.iterdir():
                if not model_dir.is_dir():
                    continue
                    
                model_name = model_dir.name
                
                # ëª¨ë¸ í•„í„°ë§
                if model_filter and model_name != model_filter:
                    continue
                    
                print(f"  ğŸ¤– ëª¨ë¸ '{model_name}' ì²˜ë¦¬ ì¤‘...")
                
                # JSON íŒŒì¼ë“¤ ì°¾ê¸°
                json_files = list(model_dir.glob("*.json"))
                print(f"    ğŸ“ {len(json_files)}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
                
                # ê° JSON íŒŒì¼ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰
                for json_file in json_files:
                    # eval íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                    eval_file = json_file.parent / f"{json_file.stem}_evaluated.json"
                    if eval_file.exists():
                        print(f"      â­ï¸ í‰ê°€ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {eval_file.name}")
                        # ê¸°ì¡´ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œë“œ
                        try:
                            stats = evaluator.analyze_evaluation_results(str(eval_file))
                            result = {
                                'method': method_name,
                                'dataset': dataset_name,
                                'model': model_name,
                                'file': json_file.name,
                                'evaluated_file': str(eval_file),
                                'stats': stats
                            }
                            dataset_results.append(result)
                            method_results.append(result)
                            all_results.append(result)
                            print(f"        âœ… ê¸°ì¡´ ê²°ê³¼ ë¡œë“œë¨ - ì •í™•ë„: {stats.get('preference_following_accuracy_percent', 0)}%")
                        except Exception as e:
                            print(f"        âš ï¸ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                            # ê¸°ì¡´ íŒŒì¼ì´ ì†ìƒëœ ê²½ìš° ìƒˆë¡œ í‰ê°€
                            continue
                    else:
                        print(f"      ğŸ”„ í‰ê°€ ì¤‘: {json_file.name}")
                        
                        try:
                            # í‰ê°€ ì‹¤í–‰ (eval íŒŒì¼ì€ gen íŒŒì¼ê³¼ ê°™ì€ ê³³ì— ìƒì„±)
                            evaluated_file = evaluator.evaluate_generation_file(str(json_file))
                            
                            if evaluated_file:
                                # ê²°ê³¼ ë¶„ì„
                                stats = evaluator.analyze_evaluation_results(evaluated_file)
                                
                                print(f"        âœ… í‰ê°€ ì™„ë£Œ - ì •í™•ë„: {stats.get('preference_following_accuracy_percent', 0)}%")
                                
                                # ê²°ê³¼ ì €ì¥
                                result = {
                                    'method': method_name,
                                    'dataset': dataset_name,
                                    'model': model_name,
                                    'file': json_file.name,
                                    'evaluated_file': evaluated_file,
                                    'stats': stats
                                }
                                
                                dataset_results.append(result)
                                method_results.append(result)
                                all_results.append(result)
                                
                            else:
                                print(f"        âŒ í‰ê°€ ì‹¤íŒ¨: {json_file.name}")
                                
                        except Exception as e:
                            print(f"        âŒ í‰ê°€ ì˜¤ë¥˜: {str(e)}")
            
            # ë°ì´í„°ì…‹ë³„ ê²°ê³¼ë¥¼ txt íŒŒì¼ë¡œ ì €ì¥
            if dataset_results:
                save_dataset_results(dataset_dir, dataset_name, dataset_results)
        
        # ë©”ì†Œë“œë³„ ì „ì²´ ê²°ê³¼ ì €ì¥
        if method_results:
            save_method_results(method_dir, method_name, method_results)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if all_results:
        save_overall_results(organized_path, all_results)

def load_existing_dataset_results(csv_file, method_name, dataset_name):
    """ê¸°ì¡´ ë°ì´í„°ì…‹ CSV íŒŒì¼ì—ì„œ ê²°ê³¼ë¥¼ ë¡œë“œ"""
    try:
        results = []
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # CSVì—ì„œ í†µê³„ ì •ë³´ë¥¼ ë³µì›
                stats = {
                    'total_responses': int(row.get('total_responses', 0)),
                    'preference_following_accuracy_percent': float(row.get('preference_following_accuracy(%)', 0)),
                    'acknowledgement_percent': float(row.get('acknowledgement(%)', 0)),
                    'violation_percent': float(row.get('violation(%)', 0)),
                    'unhelpful_percent': float(row.get('unhelpful(%)', 0)),
                    'preference_adherence_accuracy': int(row.get('preference_adherence_accuracy', 0)),
                    'acknowledgement': int(float(row.get('acknowledgement(%)', 0)) * int(row.get('total_responses', 0)) / 100) if int(row.get('total_responses', 0)) > 0 else 0,
                    'violation': int(float(row.get('violation(%)', 0)) * int(row.get('total_responses', 0)) / 100) if int(row.get('total_responses', 0)) > 0 else 0,
                    'error_unhelpful': int(float(row.get('unhelpful(%)', 0)) * int(row.get('total_responses', 0)) / 100) if int(row.get('total_responses', 0)) > 0 else 0,
                }
                
                result = {
                    'method': method_name,
                    'dataset': dataset_name,
                    'model': row.get('model', ''),
                    'file': row.get('file', ''),
                    'evaluated_file': '',  # CSVì—ì„œëŠ” eval íŒŒì¼ ê²½ë¡œë¥¼ ì•Œ ìˆ˜ ì—†ìŒ
                    'stats': stats
                }
                results.append(result)
        return results
    except Exception as e:
        print(f"    âŒ ê¸°ì¡´ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return []

def save_dataset_results(dataset_dir, dataset_name, dataset_results):
    """ë°ì´í„°ì…‹ë³„ ê²°ê³¼ë¥¼ txt íŒŒì¼ê³¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    # persona_indexë³„ ì •í™•ë„ ì¶”ì¶œ
    persona_accuracies = {}
    
    for result in dataset_results:
        # íŒŒì¼ëª…ì—ì„œ persona_index ì¶”ì¶œ (ì˜ˆ: gen_standard_flat_0.json -> 0)
        file_name = result['file']
        match = re.search(r'_(\d+)\.json$', file_name)
        if match:
            persona_index = int(match.group(1))
            accuracy = result['stats'].get('preference_following_accuracy_percent', 0)
            persona_accuracies[persona_index] = accuracy
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    csv_file = dataset_dir / f"{dataset_name}.csv"
    fieldnames = [
        "method", "dataset", "model", "file",
        "total_responses", "preference_following_accuracy(%)",
        "acknowledgement(%)", "violation(%)", "unhelpful(%)",
        "preference_adherence_accuracy"
    ]
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for result in dataset_results:
            stats = result['stats']
            writer.writerow({
                "method": result['method'],
                "dataset": result['dataset'],
                "model": result['model'],
                "file": result['file'],
                "total_responses": stats.get('total_responses', 0),
                "preference_following_accuracy(%)": stats.get('preference_following_accuracy_percent', 0),
                "acknowledgement(%)": stats.get('acknowledgement_percent', 0),
                "violation(%)": stats.get('violation_percent', 0),
                "unhelpful(%)": stats.get('unhelpful_percent', 0),
                "preference_adherence_accuracy": stats.get('preference_adherence_accuracy', 0)
            })
    
    # txt íŒŒì¼ë¡œ ì €ì¥
    txt_file = dataset_dir / f"{dataset_name}.txt"
    
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(f"Dataset: {dataset_name}\n")
        f.write("="*50 + "\n\n")
        
        # persona_indexë³„ ì •í™•ë„
        f.write("Persona Indexë³„ ì •í™•ë„:\n")
        f.write("-" * 30 + "\n")
        for persona_idx in sorted(persona_accuracies.keys()):
            accuracy = persona_accuracies[persona_idx]
            f.write(f"Persona {persona_idx:2d}: {accuracy:6.2f}%\n")
        
        # í‰ê·  ì •í™•ë„ ê³„ì‚°
        if persona_accuracies:
            avg_accuracy = sum(persona_accuracies.values()) / len(persona_accuracies)
            f.write(f"\ní‰ê·  ì •í™•ë„: {avg_accuracy:.2f}%\n")
            f.write(f"ì´ Persona ìˆ˜: {len(persona_accuracies)}\n")
        
        # ìƒì„¸ í†µê³„
        f.write(f"\nìƒì„¸ í†µê³„:\n")
        f.write("-" * 30 + "\n")
        total_responses = sum(r['stats'].get('total_responses', 0) for r in dataset_results)
        total_accuracy = sum(r['stats'].get('preference_adherence_accuracy', 0) for r in dataset_results)
        total_acknowledgement = sum(r['stats'].get('acknowledgement', 0) for r in dataset_results)
        total_violation = sum(r['stats'].get('violation', 0) for r in dataset_results)
        total_unhelpful = sum(r['stats'].get('error_unhelpful', 0) for r in dataset_results)
        
        f.write(f"ì´ ì‘ë‹µ ìˆ˜: {total_responses}\n")
        f.write(f"ì „ì²´ ì •í™•ë„: {(total_accuracy/total_responses*100):.2f}%\n" if total_responses > 0 else "ì „ì²´ ì •í™•ë„: 0.00%\n")
        f.write(f"ì¸ì •ë¥ : {(total_acknowledgement/total_responses*100):.2f}%\n" if total_responses > 0 else "ì¸ì •ë¥ : 0.00%\n")
        f.write(f"ìœ„ë°˜ë¥ : {(total_violation/total_responses*100):.2f}%\n" if total_responses > 0 else "ìœ„ë°˜ë¥ : 0.00%\n")
        f.write(f"ë„ì›€ ì•ˆë¨: {(total_unhelpful/total_responses*100):.2f}%\n" if total_responses > 0 else "ë„ì›€ ì•ˆë¨: 0.00%\n")
    
    print(f"    ğŸ’¾ ë°ì´í„°ì…‹ ê²°ê³¼ ì €ì¥: {txt_file}, {csv_file}")

def save_method_results(method_dir, method_name, method_results):
    """ë©”ì†Œë“œë³„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    csv_file = method_dir / f"eval_{method_name}_summary.csv"
    
    fieldnames = [
        "method", "dataset", "model", "file",
        "total_responses", "preference_following_accuracy(%)",
        "acknowledgement(%)", "violation(%)", "unhelpful(%)",
        "preference_adherence_accuracy"
    ]
    
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for result in method_results:
            stats = result['stats']
            writer.writerow({
                "method": result['method'],
                "dataset": result['dataset'],
                "model": result['model'],
                "file": result['file'],
                "total_responses": stats.get('total_responses', 0),
                "preference_following_accuracy(%)": stats.get('preference_following_accuracy_percent', 0),
                "acknowledgement(%)": stats.get('acknowledgement_percent', 0),
                "violation(%)": stats.get('violation_percent', 0),
                "unhelpful(%)": stats.get('unhelpful_percent', 0),
                "preference_adherence_accuracy": stats.get('preference_adherence_accuracy', 0)
            })
    
    print(f"  ğŸ’¾ ë©”ì†Œë“œë³„ ê²°ê³¼ ì €ì¥: {csv_file}")

def save_overall_results(organized_path, all_results):
    """ì „ì²´ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    overall_csv_file = organized_path / "eval_all_results.csv"
    
    fieldnames = [
        "method", "dataset", "model", "file",
        "total_responses", "preference_following_accuracy(%)",
        "acknowledgement(%)", "violation(%)", "unhelpful(%)",
        "preference_adherence_accuracy"
    ]
    
    with open(overall_csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for result in all_results:
            stats = result['stats']
            writer.writerow({
                "method": result['method'],
                "dataset": result['dataset'],
                "model": result['model'],
                "file": result['file'],
                "total_responses": stats.get('total_responses', 0),
                "preference_following_accuracy(%)": stats.get('preference_following_accuracy_percent', 0),
                "acknowledgement(%)": stats.get('acknowledgement_percent', 0),
                "violation(%)": stats.get('violation_percent', 0),
                "unhelpful(%)": stats.get('unhelpful_percent', 0),
                "preference_adherence_accuracy": stats.get('preference_adherence_accuracy', 0)
            })
    
    print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥: {overall_csv_file}")
    
    # ì „ì²´ í†µê³„ ê³„ì‚°
    total_responses = sum(r['stats'].get('total_responses', 0) for r in all_results)
    total_accuracy = sum(r['stats'].get('preference_adherence_accuracy', 0) for r in all_results)
    total_acknowledgement = sum(r['stats'].get('acknowledgement', 0) for r in all_results)
    total_violation = sum(r['stats'].get('violation', 0) for r in all_results)
    total_unhelpful = sum(r['stats'].get('error_unhelpful', 0) for r in all_results)
    
    overall_accuracy_percent = round((total_accuracy / total_responses) * 100, 2) if total_responses > 0 else 0
    overall_acknowledgement_percent = round((total_acknowledgement / total_responses) * 100, 2) if total_responses > 0 else 0
    overall_violation_percent = round((total_violation / total_responses) * 100, 2) if total_responses > 0 else 0
    overall_unhelpful_percent = round((total_unhelpful / total_responses) * 100, 2) if total_responses > 0 else 0
    
    print(f"\nğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼:")
    print(f"ì „ì²´ ì‘ë‹µ ìˆ˜: {total_responses}")
    print(f"ì „ì²´ ì„ í˜¸ë„ ì¤€ìˆ˜ ì •í™•ë„: {overall_accuracy_percent}%")
    print(f"ì „ì²´ ì¸ì •ë¥ : {overall_acknowledgement_percent}%")
    print(f"ì „ì²´ ìœ„ë°˜ë¥ : {overall_violation_percent}%")
    print(f"ì „ì²´ ë„ì›€ ì•ˆë¨: {overall_unhelpful_percent}%")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ìë™ í‰ê°€ ì‹œìŠ¤í…œ')
    parser.add_argument('--organized_dir', type=str, default='organized_genfiles',
                       help='ì •ë¦¬ëœ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬')
    parser.add_argument('--method', type=str, default=None,
                       help='íŠ¹ì • ë©”ì†Œë“œë§Œ í‰ê°€ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--dataset', type=str, default=None,
                       help='íŠ¹ì • ë°ì´í„°ì…‹ë§Œ í‰ê°€ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--model', type=str, default=None,
                       help='íŠ¹ì • ëª¨ë¸ë§Œ í‰ê°€ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--vllm_url', type=str, default='http://localhost:8011', help='vLLM ì„œë²„ URL')
    parser.add_argument('--eval_model', type=str, default='meta-llama/Llama-3.3-70B-Instruct', help='í‰ê°€ ëª¨ë¸')
    
    args = parser.parse_args()
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = EvaluationWithDifferentLLM(
        vllm_base_url=args.vllm_url,
        evaluation_model=args.eval_model,
        max_tokens=512,
        temperature=0.0,
        timeout=60,
        retry_count=3
    )
    
    # ì •ë¦¬ëœ íŒŒì¼ë“¤ í‰ê°€
    print("ğŸš€ ì •ë¦¬ëœ íŒŒì¼ë“¤ì— ëŒ€í•œ í‰ê°€ ì‹œì‘")
    print(f"ğŸ“ ì •ë¦¬ëœ í´ë”: {args.organized_dir}")
    
    if args.method:
        print(f"ğŸ” ë©”ì†Œë“œ í•„í„°: {args.method}")
    if args.dataset:
        print(f"ğŸ“Š ë°ì´í„°ì…‹ í•„í„°: {args.dataset}")
    if args.model:
        print(f"ğŸ¤– ëª¨ë¸ í•„í„°: {args.model}")
    
    evaluate_organized_files(
        args.organized_dir, 
        evaluator, 
        method_filter=args.method,
        dataset_filter=args.dataset,
        model_filter=args.model
    )

if __name__ == "__main__":
    main() 